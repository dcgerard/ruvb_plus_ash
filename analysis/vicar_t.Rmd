---
title: "explore vicar package; t distribution"
author: "Matthew Stephens"
date: "`r Sys.Date()`"
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

# Summary

I re-run MS's code a few times. The rest is code that I used to debug a problem. It is now solved, so it doesn't make much sense here.

# Simulate data

Run MS's code a few times. This is in all-null setting with t-errors.

```{r}
library("vicar")
set.seed(3)
itermax <- 30

result_mat <- matrix(NA, nrow = itermax, ncol = 8)
colnames(result_mat) <- c("pi0_ash_norm", "pi0_ash_t", "pi0_mouth_norm",
                          "pi0_mouth_t", "pi0_mouth_norm_cal", "pi0_mouth_t_cal",
                          "xi_norm", "xi_t")

for (index in 1:itermax) {
  n=1000
  bhat = rt(n,df=4) # t with 4 df
  shat = rep(1,n)
  library(ashr)
  bhat.ash.t4 = ash(bhat,shat,df = 4)
  bhat.ash.norm = ash(bhat,shat)
  result_mat[index, 1] <- get_pi0(bhat.ash.norm)
  result_mat[index, 2] <- get_pi0(bhat.ash.t4)
  
  a = matrix(rep(1,n),nrow = n, ncol=1) # just put in an "intercept" confounder with no effect
  a_seq = bhat.ash.norm$fitted_g$a
  b_seq = bhat.ash.norm$fitted_g$b
  lambda_seq = rep(1,length(a_seq))
  lambda_seq[1] = 10
  bhat.m.norm = mouthwash_second_step(bhat,shat,a,lambda_seq = lambda_seq,a_seq = a_seq,
                                      b_seq=b_seq,mixing_dist = "uniform", likelihood="normal",
                                      scale_var = FALSE)
  bhat.m.t4 = mouthwash_second_step(bhat,shat,a,lambda_seq = lambda_seq,a_seq = a_seq,
                                    b_seq=b_seq,mixing_dist = "uniform", likelihood="t",
                                    scale_var = FALSE, degrees_freedom = 4)
  bhat.m.norm.c = mouthwash_second_step(bhat,shat,a,lambda_seq = lambda_seq,a_seq = a_seq,
                                        b_seq=b_seq,mixing_dist = "uniform",
                                        likelihood="normal",
                                        scale_var = TRUE)
  bhat.m.t4.c = mouthwash_second_step(bhat,shat,a,lambda_seq = lambda_seq,a_seq = a_seq,
                                      b_seq=b_seq,mixing_dist = "uniform", likelihood="t",
                                      scale_var = TRUE, degrees_freedom = 4)

  result_mat[index, 3] <- bhat.m.norm$pi0
  result_mat[index, 4] <- bhat.m.t4$pi0
  result_mat[index, 5] <- bhat.m.norm.c$pi0
  result_mat[index, 6] <- bhat.m.t4.c$pi0
  result_mat[index, 7] <- bhat.m.norm.c$xi
  result_mat[index, 8] <- bhat.m.t4.c$xi
}
```

Plot results. t does indeed do best. Variance inflation term seems to make things worse.

```{r}
library(tidyverse)
library(vicar)
library(ashr)
dat <- as_data_frame(result_mat) %>% select(contains("pi0_")) %>%
  gather(key = "method", value = "pi0")
dat$method <- stringr::str_replace(dat$method, "pi0_", "")
ggplot(data = dat, mapping = aes(x = method, y = pi0)) +
  geom_boxplot()
```

In real data, it is very often the case that xi is less than 1.

```{r}
dat_xi <- as_data_frame(result_mat) %>% select(contains("xi")) %>%
  gather(key = "dist", value = "xi")
dat_xi$dist <- stringr::str_replace(dat_xi$dist, "xi_", "")
ggplot(data = dat_xi, mapping = aes(x = dist, y = xi)) +
  geom_boxplot() +
  geom_hline(yintercept = 1, lty = 2)
```


# Proof that large df equals Inf df.

He was using `df = 100000` instead of `df = Inf`. Is there a big difference?



```{r}
  set.seed(5)
  n=1000
  bhat = rt(n,df=4) # t with 4 df
  shat = rep(1,n)
  bhat.ash.t4 = ash(bhat,shat,df = 4)
  bhat.ash.norm = ash(bhat,shat)
  result_mat[index, 1] <- get_pi0(bhat.ash.norm)
  result_mat[index, 2] <- get_pi0(bhat.ash.t4)
  
  a = matrix(rep(1,n),nrow = n, ncol=1) # just put in an "intercept" confounder with no effect
  a_seq = bhat.ash.norm$fitted_g$a
  b_seq = bhat.ash.norm$fitted_g$b
  lambda_seq = rep(1,length(a_seq))
  lambda_seq[1] = 10
  bhat.m.norm.c = mouthwash_second_step(bhat,shat,a,lambda_seq = lambda_seq,a_seq = a_seq,
                                        b_seq=b_seq,mixing_dist = "uniform", 
                                        likelihood="t",
                                        scale_var = TRUE, degrees_freedom = Inf)
  bhat.m.t100.c = mouthwash_second_step(bhat,shat,a,lambda_seq = lambda_seq,a_seq = a_seq,
                                      b_seq=b_seq,mixing_dist = "uniform", likelihood="t",
                                      scale_var = TRUE, degrees_freedom = 100)
  bhat.m.t1000.c = mouthwash_second_step(bhat,shat,a,lambda_seq = lambda_seq,a_seq = a_seq,
                                      b_seq=b_seq,mixing_dist = "uniform", likelihood="t",
                                      scale_var = TRUE, degrees_freedom = 1000)
  bhat.m.t10000.c = mouthwash_second_step(bhat,shat,a,lambda_seq = lambda_seq,a_seq = a_seq,
                                      b_seq=b_seq,mixing_dist = "uniform", likelihood="t",
                                      scale_var = TRUE, degrees_freedom = 10000)
  bhat.m.t100000.c = mouthwash_second_step(bhat,shat,a,lambda_seq = lambda_seq,a_seq = a_seq,
                                      b_seq=b_seq,mixing_dist = "uniform", likelihood="t",
                                      scale_var = TRUE, degrees_freedom = 100000)
  bhat.m.norm.c$xi
  bhat.m.t100.c$xi
  bhat.m.t1000.c$xi
  bhat.m.t10000.c$xi
  bhat.m.t100000.c$xi
```


# The rest of this is code that I used to debug a problem I had for large df vs Inf df.

# I don't think ashr is the problem.

Here is an example internal dataset from `mouthwash_second_step`

```{r}
example_list <- readRDS(file = "../data/example_list.RDS")
str(example_list)
```

I'll run `ashr.workhorse` under the settings with `df = NULL` and `df = 100000`.

```{r}
val1 <- ashr::ash.workhorse(betahat = example_list$betahat,
                            sebetahat = example_list$sebetahat,
                            df = NULL,
                            prior = "nullbiased",
                            nullweight = 10,
                            g = example_list$g,
                            fixg = FALSE,
                            mixcompdist = "halfuniform",
                            alpha = 0)
ashr::get_pi0(val1)

val2 <-  ashr::ash.workhorse(betahat = example_list$betahat,
                            sebetahat = example_list$sebetahat,
                            df = 100000,
                            prior = "nullbiased",
                            nullweight = 10,
                            g = example_list$g,
                            fixg = FALSE,
                            mixcompdist = "halfuniform",
                            alpha = 0)

max(val2$fitted_g$pi - val1$fitted_g$pi)
```


# What about my function `uniform_mix_llike`?

The log-likelihood does not seem to be at fault here.

```{r}
rm(list = ls())
set.seed(991)

p <- 103
k <- 3
S_diag <- stats::rchisq(p, 5) / 5
alpha_tilde <- matrix(stats::rnorm(k * p), nrow = p)
z2 <- matrix(stats::rnorm(k), ncol = 1)
beta <- matrix(stats::rnorm(p), ncol = 1)
betahat_ols <- beta + alpha_tilde %*% z2 + rnorm(p, mean = 0, sd = sqrt(S_diag))

M             <- 23
a_seq         <- seq(-10, 0, length = M)
b_seq         <- seq(10, 0, length = M)
lambda_seq    <- rep(1, M)
lambda_seq[length(lambda_seq)] <- 10
pi_vals <- rep(1 / M, length = M)
xi <- 1
degrees_freedom <- 3
scale_var <- TRUE
pi_init_type <- "zero_conc"

df_vec <- c(1:10, 100, 1000, 10000, Inf)
llike_vec <- rep(NA, length = length(df_vec))
for (index in 1:length(df_vec)) {
  llike_vec[index] <- vicar:::uniform_mix_llike(pi_vals = pi_vals, z2 = z2, xi = xi,
                                betahat_ols = betahat_ols, S_diag = S_diag,
                                alpha_tilde = alpha_tilde, a_seq = a_seq, b_seq = b_seq,
                                lambda_seq = lambda_seq,
                                degrees_freedom = df_vec[index])
}

plot(log(df_vec), llike_vec, xlab = "Log DF", ylab = "Log-likelihood")
abline(h = llike_vec[df_vec == Inf], lty = 2)
```

# What about optimizing the scale?

```{r}
mout1 <- vicar:::mouthwash_coordinate(pi_init = pi_vals, z_init = z2, xi_init = 1, 
                     betahat_ols = betahat_ols, S_diag = S_diag,
                     alpha_tilde = alpha_tilde, a_seq = a_seq, 
                     b_seq = b_seq, lambda_seq = lambda_seq,
                     degrees_freedom = Inf, scale_var = TRUE,
                     tol = 10 ^ -6, maxit = 100, plot_update = FALSE)
mout2 <- vicar:::mouthwash_coordinate(pi_init = pi_vals, z_init = z2, xi_init = 1, 
                     betahat_ols = betahat_ols, S_diag = S_diag,
                     alpha_tilde = alpha_tilde, a_seq = a_seq, 
                     b_seq = b_seq, lambda_seq = lambda_seq,
                     degrees_freedom = 100000, scale_var = TRUE,
                     tol = 10 ^ -6, maxit = 100, plot_update = FALSE)

mout1
mout2
```


The scale estimate part seems to be OK

```{r}
optim_out1 <- stats::optim(par = 1, fn = vicar:::uniform_mix_llike, method = "Brent",
                           lower = 10 ^ -14, upper = 10,
                           pi_vals = pi_vals, z2 = z2, betahat_ols = betahat_ols,
                           S_diag = S_diag, alpha_tilde = alpha_tilde, a_seq = a_seq,
                           b_seq = b_seq, lambda_seq = lambda_seq,
                           degrees_freedom = Inf,
                           control = list(fnscale = -1))

optim_out2 <- stats::optim(par = 1, fn = vicar:::uniform_mix_llike, method = "Brent",
                           lower = 10 ^ -14, upper = 10,
                           pi_vals = pi_vals, z2 = z2, betahat_ols = betahat_ols,
                           S_diag = S_diag, alpha_tilde = alpha_tilde, a_seq = a_seq,
                           b_seq = b_seq, lambda_seq = lambda_seq,
                           degrees_freedom = 10000,
                           control = list(fnscale = -1))

optim_out1$par
optim_out2$par
```

The problem seems to be with the step where I estimate the confounders

```{r}
optim_out1 <- stats::optim(par = z2, fn = vicar:::uniform_mix_llike,
                          gr = vicar:::mouthwash_z_grad, method = "BFGS",
                          pi_vals = pi_vals, xi = xi, betahat_ols = betahat_ols,
                          S_diag = S_diag, alpha_tilde = alpha_tilde, a_seq = a_seq,
                          b_seq = b_seq, lambda_seq = lambda_seq,
                          degrees_freedom = Inf,
                          control = list(fnscale = -1))

optim_out2 <- stats::optim(par = z2, fn = vicar:::uniform_mix_llike,
                          gr = vicar:::mouthwash_z_grad, method = "BFGS",
                          pi_vals = pi_vals, xi = xi, betahat_ols = betahat_ols,
                          S_diag = S_diag, alpha_tilde = alpha_tilde, a_seq = a_seq,
                          b_seq = b_seq, lambda_seq = lambda_seq,
                          degrees_freedom = 100000,
                          control = list(fnscale = -1))

optim_out1$par
optim_out2$par

optim_out1$value
optim_out2$value
```

# Look at gradient to see if wrong

```{r}
rm(list = ls())
set.seed(991)

p <- 103
k <- 3
S_diag <- stats::rchisq(p, 5) / 5
alpha_tilde <- matrix(stats::rnorm(k * p), nrow = p)
z2 <- matrix(stats::rnorm(k), ncol = 1)
beta <- matrix(stats::rnorm(p), ncol = 1)
betahat_ols <- beta + alpha_tilde %*% z2 + rnorm(p, mean = 0, sd = sqrt(S_diag))

M             <- 23
a_seq         <- seq(-10, 0, length = M)
b_seq         <- seq(10, 0, length = M)
lambda_seq    <- rep(1, M)
lambda_seq[length(lambda_seq)] <- 10
pi_vals <- rep(1 / M, length = M)
xi <- 1
degrees_freedom <- 3
scale_var <- TRUE
pi_init_type <- "zero_conc"

df_vec <- c(1:10, 100, 1000, 10000, Inf)
grad_matrix <- matrix(NA, nrow = length(df_vec), ncol = k)
for (index in 1:length(df_vec)) {
  grad_matrix[index, ] <- vicar:::mouthwash_z_grad(pi_vals = pi_vals, z2 = z2, xi = xi,
                                betahat_ols = betahat_ols, S_diag = S_diag,
                                alpha_tilde = alpha_tilde, a_seq = a_seq, b_seq = b_seq,
                                lambda_seq = lambda_seq,
                                degrees_freedom = df_vec[index])
}

grad_matrix
```

## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
